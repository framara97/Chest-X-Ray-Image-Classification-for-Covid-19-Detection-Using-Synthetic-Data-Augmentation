{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "# Image processing\n",
    "import imageio\n",
    "import cv2\n",
    "import skimage.transform\n",
    "\n",
    "\n",
    "# Charts\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ML, statistics\n",
    "import scipy\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, roc_auc_score, ConfusionMatrixDisplay\n",
    "\n",
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Input, Conv2D, Dense, Flatten, Dropout, Activation, BatchNormalization, MaxPooling2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SETTINGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "plt.style.use('fivethirtyeight')\n",
    "#plt.style.use('seaborn')\n",
    "\n",
    "# toy=True - development mode, small samples, limited training, fast run\n",
    "# toy=False - full data, slow learning and run\n",
    "toy=False\n",
    "\n",
    "#set size of validation set and test set in percentage\n",
    "validation_size = 10\n",
    "test_size = 10\n",
    "\n",
    "# Generators Paramenters\n",
    "batch_size = 32\n",
    "img_size = (224,224)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Root Path\n",
    "dataset_path = \"../input/covid19-radiography-database/COVID-19_Radiography_Dataset\"\n",
    "\n",
    "# Folder Paths\n",
    "covid_path = dataset_path + \"/COVID\"\n",
    "lungOpacity_path = dataset_path + \"/Lung_Opacity\"\n",
    "normal_path = dataset_path + \"/Normal\"\n",
    "viralPneumonia_path = dataset_path + \"/Viral Pneumonia\"\n",
    "\n",
    "# LABELS LOCAL Array\n",
    "#labels = ['COVID', 'Lung Opacity', 'Normal', 'Viral Pneumonia']\n",
    "labels = ['COVID', 'Normal', 'Viral Pneumonia']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Filename list\n",
    "def filename_list():\n",
    "\n",
    "    if toy :\n",
    "        random.seed(100)\n",
    "        filenames = random.sample(os.listdir(covid_path), 200) + random.sample(os.listdir(lungOpacity_path), 200) + random.sample(os.listdir(normal_path), 200) + random.sample(os.listdir(viralPneumonia_path), 200)\n",
    "    else :\n",
    "       #filenames = os.listdir(covid_path) + os.listdir(lungOpacity_path) + os.listdir(normal_path) + os.listdir(viralPneumonia_path)\n",
    "        filenames = os.listdir(covid_path) + os.listdir(normal_path) + os.listdir(viralPneumonia_path)\n",
    "\n",
    "    return filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_generation(files):\n",
    "    \n",
    "    categories = []\n",
    "    for filename in files:\n",
    "        category = filename.split('-')[0]\n",
    "        if category == labels[0]:\n",
    "            categories.append(str(0))\n",
    "        elif category == labels[1]:\n",
    "            categories.append(str(1))\n",
    "        elif category == labels[2]:\n",
    "            categories.append(str(2))\n",
    "        #elif category == labels[3]:\n",
    "         #   categories.append(str(3))\n",
    "\n",
    "    for i in range(len(files)):\n",
    "        if labels[0] in files[i]:\n",
    "            files[i] = os.path.join(covid_path, files[i])\n",
    "        #elif labels[1] in files[i]:\n",
    "            #files[i] = os.path.join(lungOpacity_path, files[i])\n",
    "        elif labels[1] in files[i]:\n",
    "            files[i] = os.path.join(normal_path, files[i])\n",
    "        elif labels[2] in files[i]:\n",
    "            files[i] = os.path.join(viralPneumonia_path, files[i])\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'filename': files,\n",
    "        'category': categories\n",
    "    })\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = filename_list()\n",
    "dataset_df = dataframe_generation(filenames)\n",
    "\n",
    "dataset_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_img(id):\n",
    "    \"\"\"\n",
    "    Read image by it's id\n",
    "    \"\"\"\n",
    "    file=id\n",
    "    im=cv2.imread(file)\n",
    "    return im\n",
    "\n",
    "def draw_sample_images(dataframe, ncols, labels):\n",
    "\n",
    "        ncols=ncols\n",
    "        n_rows = len(labels)\n",
    "\n",
    "        f, ax = plt.subplots(nrows=n_rows,ncols=ncols, \n",
    "                             figsize=(4*ncols,5*2))\n",
    "        i=-1\n",
    "        captions=labels\n",
    "        \n",
    "        for label in range(n_rows):\n",
    "            i=i+1\n",
    "            samples = dataframe[dataframe[\"category\"].astype(int) == label].filename.sample(ncols).values\n",
    "            for j in range(0,ncols):\n",
    "                file_id=samples[j]\n",
    "                im=read_img(file_id)\n",
    "                ax[i, j].imshow(im)\n",
    "                ax[i, j].set_title(captions[i], fontsize=16)  \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data_distribution(dataframe):\n",
    "    dataframe.category.value_counts().plot(kind='bar')\n",
    "    plt.title('Dataset Distribution')\n",
    "    plt.xlabel('Label')\n",
    "    plt.ylabel('Count')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_sample_images(dataset_df, 4, labels)\n",
    "plot_data_distribution(dataset_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_reduction(size):\n",
    "    random.seed(100)\n",
    "    reduced_dataset = random.sample(os.listdir(covid_path), size) + random.sample(os.listdir(normal_path), size) + random.sample(os.listdir(viralPneumonia_path), size)\n",
    "\n",
    "    df_reduced = dataframe_generation(reduced_dataset)\n",
    "\n",
    "    return df_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Reduced = data_reduction(1000)\n",
    "plot_data_distribution(df_Reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split in Train, Validation and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sets_split(dataframe):\n",
    "    x_1_split = (validation_size + test_size)/100\n",
    "    x_2_split = round(test_size / (validation_size + test_size), 2)\n",
    "\n",
    "    train_df, valTest_df = train_test_split(dataframe, test_size=x_1_split, random_state=24)\n",
    "    val_df, test_df = train_test_split(valTest_df, test_size=x_2_split, random_state=24)\n",
    "\n",
    "    return train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validation, test = sets_split(dataset_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_datagen = ImageDataGenerator(\n",
    "            samplewise_center=True, #Set each sample mean to 0.\n",
    "            samplewise_std_normalization= True, # Divide each input by its standard deviation]     \n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = _datagen.flow_from_dataframe(\n",
    "            dataframe=train,\n",
    "            x_col=\"filename\",\n",
    "            y_col=\"category\",\n",
    "            batch_size=batch_size,\n",
    "            seed=42,\n",
    "            shuffle=True,\n",
    "            class_mode=\"categorical\",\n",
    "            color_mode=\"rgb\",\n",
    "            target_size=img_size)\n",
    "print('Train generator created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_generator = _datagen.flow_from_dataframe(\n",
    "            dataframe=validation,\n",
    "            x_col=\"filename\",\n",
    "            y_col=\"category\",\n",
    "            batch_size=batch_size,\n",
    "            seed=42,\n",
    "            shuffle=True,\n",
    "            class_mode=\"categorical\",\n",
    "            color_mode=\"rgb\",\n",
    "            target_size=img_size)    \n",
    "print('Validation generator created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_test_datagen=ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_generator = _test_datagen.flow_from_dataframe(\n",
    "            dataframe=test,\n",
    "            x_col=\"filename\",\n",
    "            y_col=None,\n",
    "            class_mode=None,\n",
    "            batch_size=batch_size,\n",
    "            seed=42,\n",
    "            shuffle=False,\n",
    "            color_mode=\"rgb\",\n",
    "            target_size=img_size)     \n",
    "print('Test generator created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL GENERATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics Aux Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(size):\n",
    "        base_model = ResNet50(input_shape= (size,size,3), include_top=False)\n",
    "\n",
    "        x=base_model.output\n",
    "        x=GlobalAveragePooling2D()(x)\n",
    "        x = Dense(128, activation='relu')(x)\n",
    "        x = Dense(64, activation='relu')(x)\n",
    "        out=Dense(3,activation='softmax')(x)\n",
    "\n",
    "        model=Model(inputs=base_model.input,outputs=out) \n",
    "            \n",
    "        model.compile(optimizer= Adam(0.0001),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy', f1_m, precision_m, recall_m])\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(224)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data, val_data, toy):\n",
    "        \"\"\"\n",
    "        Train the model\n",
    "        \"\"\"\n",
    "        if toy:\n",
    "            epochs=3\n",
    "\n",
    "        else:\n",
    "            epochs=100\n",
    "            \n",
    "        # We'll stop training if no improvement after some epochs\n",
    "        earlystopper = EarlyStopping(monitor='val_loss', mode='min', patience=5 , restore_best_weights=True)\n",
    "\n",
    "        # Save the best model during the traning\n",
    "        checkpointer = ModelCheckpoint('best_model1.h5'\n",
    "                                        ,monitor='val_loss'\n",
    "                                        ,mode='min'\n",
    "                                        ,verbose=1\n",
    "                                        ,save_best_only=True)\n",
    "        # Train\n",
    "        training = model.fit(train_data, \n",
    "                            validation_data = val_data, \n",
    "                            epochs = epochs, \n",
    "                            verbose = 1,\n",
    "                            batch_size = batch_size,\n",
    "                            callbacks=[earlystopper, checkpointer])\n",
    "        \n",
    "        print(\"Trained\")\n",
    "                            \n",
    "        return training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained = train(model, train_generator, val_generator, toy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL EVALUATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_raw = model.predict(test_generator)\n",
    "y_pred = np.argmax(y_pred_raw, axis=1)\n",
    "y_true = test.category.values\n",
    "y_true=y_true[:len(y_pred)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics Plot Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(training):\n",
    "        \"\"\"\n",
    "        Plot training history\n",
    "        \"\"\"\n",
    "        ## Trained model analysis and evaluation\n",
    "        f, ax = plt.subplots(1,2, figsize=(12,3))\n",
    "        ax[0].plot(training.history['loss'], label=\"Loss\")\n",
    "        ax[0].plot(training.history['val_loss'], label=\"Validation loss\")\n",
    "        ax[0].set_title('Loss')\n",
    "        ax[0].set_xlabel('Epoch')\n",
    "        ax[0].set_ylabel('Loss')\n",
    "        ax[0].legend()\n",
    "\n",
    "        # Accuracy\n",
    "        ax[1].plot(training.history['accuracy'], label=\"Accuracy\")\n",
    "        ax[1].plot(training.history['val_accuracy'], label=\"Validation accuracy\")\n",
    "        ax[1].set_title('Accuracy')\n",
    "        ax[1].set_xlabel('Epoch')\n",
    "        ax[1].set_ylabel('Accuracy')\n",
    "        ax[1].legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def train_report(training):\n",
    "    print(\"------- Training Report -----------\")\n",
    "    print(\"Accuracy: %0.2f\" % (np.mean(training.history['accuracy'])*100))\n",
    "    print(\"F1 Score: %0.2f\" % (np.mean(training.history['f1_m'])*100))\n",
    "    print(\"Precision: %0.2f\" % (np.mean(training.history['precision_m'])*100))\n",
    "    print(\"Recall: %0.2f\" % (np.mean(training.history['recall_m'])*100))\n",
    "\n",
    "    print(\"------- Validation Report -----------\")\n",
    "    print(\"Accuracy: %0.2f\" % (np.mean(training.history['val_accuracy'])*100))\n",
    "    print(\"F1 Score: %0.2f\" % (np.mean(training.history['val_f1_m'])*100))\n",
    "    print(\"Precision: %0.2f\" % (np.mean(training.history['val_precision_m'])*100))\n",
    "    print(\"Recall: %0.2f\" % (np.mean(training.history['val_recall_m'])*100))\n",
    "\n",
    "def print_report():\n",
    "    \"\"\"\n",
    "    Predict and evaluate using ground truth from labels\n",
    "    Test generator did not shuffle \n",
    "    and we can use true labels for comparison\n",
    "    \"\"\"\n",
    "    #Print classification report\n",
    "    print(metrics.classification_report(y_true, y_pred.astype(str), target_names = labels))\n",
    "    \n",
    "def print_conf_mtx():\n",
    "    \n",
    "    cm = confusion_matrix(y_true.astype(int), y_pred)\n",
    "\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0,1,2])\n",
    "\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.show()\n",
    "\n",
    "def multiclass_roc_auc_score(y_test, y_pred, average=\"weighted\"):\n",
    "    fig, c_ax = plt.subplots(1,1, figsize = (12, 8))\n",
    "    lb = LabelBinarizer()\n",
    "    lb.fit(y_test)\n",
    "    y_test = lb.transform(y_test)\n",
    "    y_pred = lb.transform(y_pred)\n",
    "\n",
    "    for (idx, c_label) in enumerate(labels):\n",
    "        fpr, tpr, thresholds = roc_curve(y_test[:,idx].astype(int), y_pred[:,idx])\n",
    "        c_ax.plot(fpr, tpr, label = '%s (AUC:%0.2f)'  % (c_label, auc(fpr, tpr)))\n",
    "        \n",
    "    c_ax.plot(fpr, fpr, 'b-', label = 'Random Guessing')\n",
    "    c_ax.legend()\n",
    "    c_ax.set_xlabel('False Positive Rate')\n",
    "    c_ax.set_ylabel('True Positive Rate')\n",
    "    c_ax.set_title('ROC Curve')\n",
    "    print('ROC AUC Score:', roc_auc_score(y_test, y_pred, average=average))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_report(trained)\n",
    "plot_history(trained)\n",
    "multiclass_roc_auc_score(y_true.astype(int), y_pred)\n",
    "print_report()\n",
    "print_conf_mtx()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
